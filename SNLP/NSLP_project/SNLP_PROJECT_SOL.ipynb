{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyttsx3\n",
    "# text = pyttsx3.init()\n",
    "# text.setProperty('rate', 150)\n",
    "# text.setProperty('volume', 1) # 0 to 1\n",
    "# s = '''DOMAIN: Digital content management\n",
    "# • CONTEXT: Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc.\n",
    "# are written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a\n",
    "# classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem.\n",
    "# • DATA DESCRIPTION: Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of\n",
    "# 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or\n",
    "# approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and\n",
    "# the blogger’s self-provided gender, age, industry, and astrological sign. (All are labelled for gender and age but for many, industry and/or sign is\n",
    "# marked as unknown.) All bloggers included in the corpus fall into one of three age groups:\n",
    "# • 8240 \"10s\" blogs (ages 13-17),\n",
    "# • 8086 \"20s\" blogs(ages 23-27) and\n",
    "# • 2994 \"30s\" blogs (ages 33-47)\n",
    "# • For each age group, there is an equal number of male and female bloggers. Each blog in the corpus includes at least 200 occurrences of\n",
    "# common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the\n",
    "# date of the following post and links within a post are denoted by the label url link.\n",
    "# • PROJECT OBJECTIVE: To build a NLP classifier which can use input text parameters to determine the label/s of the blog. Specific to this case\n",
    "# study, you can consider the text of the blog: ‘text’ feature as independent variable and ‘topic’ as dependent variable.\n",
    "# Steps and tasks: [ Total Score: 40 Marks]\n",
    "# 1. Read and Analyse Dataset. [5 Marks]\n",
    "# A. Clearly write outcome of data analysis(Minimum 2 points) [2 Marks]\n",
    "# B. Clean the Structured Data [3 Marks]\n",
    "# i. Missing value analysis and imputation. [1 Marks]\n",
    "# ii. Eliminate Non-English textual data. [2 Marks]\n",
    "# Hint: Refer ‘langdetect’ library to detect language of the input text)\n",
    "# 2. Preprocess unstructured data to make it consumable for model training. [5 Marks]\n",
    "# A. Eliminate All special Characters and Numbers [2 Marks]\n",
    "# B. Lowercase all textual data [1 Marks]\n",
    "# C. Remove all Stopwords [1 Marks]\n",
    "# D. Remove all extra white spaces [1 Marks]\n",
    "# 3. Build a base Classification model [8 Marks]\n",
    "# A. Create dependent and independent variables [2 Marks]\n",
    "# Hint: Treat ‘topic’ as a Target variable.\n",
    "# B. Split data into train and test. [1 Marks]\n",
    "# C. Vectorize data using any one vectorizer. [2 Marks]\n",
    "# D. Build a base model for Supervised Learning - Classification. [2 Marks]\n",
    "# E. Clearly print Performance Metrics. [1 Marks]\n",
    "# Hint: Accuracy, Precision, Recall, ROC-AUC\n",
    "# 4. Improve Performance of model. [14 Marks]\n",
    "# A. Experiment with other vectorisers. [4 Marks]\n",
    "# B. Build classifier Models using other algorithms than base model. [4 Marks]\n",
    "# C. Tune Parameters/Hyperparameters of the model/s. [4 Marks]\n",
    "# D. Clearly print Performance Metrics. [2 Marks]\n",
    "# Hint: Accuracy, Precision, Recall, ROC-AUC\n",
    "\n",
    "# 5. Share insights on relative performance comparison [8 Marks]\n",
    "# A. Which vectorizer performed better? Probable reason?.\n",
    "# [2 Marks]\n",
    "# B. Which model outperformed? Probable reason? [2 Marks]\n",
    "# C. Which parameter/hyperparameter significantly helped\n",
    "# to improve performance?Probable reason?. [2 Marks]\n",
    "# D. According to you, which performance metric should be\n",
    "# given most importance, why?. [2 Marks]'''\n",
    "# text.say(s)\n",
    "# text.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # PART A - 40 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • DOMAIN: \n",
    "        Digital content management\n",
    "### • CONTEXT:\n",
    "        Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc. are written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem. \n",
    "### • DATA DESCRIPTION: \n",
    "        Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry, and astrological sign. (All are labelled for gender and age but for many, industry and/or sign is\n",
    "        marked as unknown.) All bloggers included in the corpus fall into one of three age groups:\n",
    "            • 8240 \"10s\" blogs (ages 13-17),\n",
    "            • 8086 \"20s\" blogs(ages 23-27) and\n",
    "            • 2994 \"30s\" blogs (ages 33-47)\n",
    "            • For each age group, there is an equal number of male and female bloggers. Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label url link.\n",
    "### • PROJECT OBJECTIVE: \n",
    "        To build a NLP classifier which can use input text parameters to determine the label/s of the blog. Specific to this case study, you can consider the text of the blog: ‘text’ feature as independent variable and ‘topic’ as dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps and tasks: [ Total Score: 40 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read and Analyse Dataset. [5 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('blogs.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('E:\\\\RNN\\\\blogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import contractions_dict\n",
    "import unicodedata\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4623</th>\n",
       "      <td>766556</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>28,July,2004</td>\n",
       "      <td>Party Pooper  I have been staying up l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td>yay!  the blog is finally posting. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>3543234</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>04,June,2004</td>\n",
       "      <td>New blog created where me and Jrago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>3384037</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Scorpio</td>\n",
       "      <td>19,May,2004</td>\n",
       "      <td>damn it...April's been medita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3022585</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>Education</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>01,August,2004</td>\n",
       "      <td>Do you remember watching music videos? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>3568056</td>\n",
       "      <td>male</td>\n",
       "      <td>17</td>\n",
       "      <td>Sports-Recreation</td>\n",
       "      <td>Capricorn</td>\n",
       "      <td>01,August,2004</td>\n",
       "      <td>i am sorry for the redundancy (iron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td>woop woop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender  age              topic         sign            date  \\\n",
       "4623   766556  female   34             indUnk  Sagittarius    28,July,2004   \n",
       "2201   589736    male   35         Technology        Aries  05,August,2004   \n",
       "3849  3543234    male   14             indUnk      Scorpio    04,June,2004   \n",
       "3929  3384037  female   17            Student      Scorpio     19,May,2004   \n",
       "485   3022585  female   27          Education     Aquarius  01,August,2004   \n",
       "889   3568056    male   17  Sports-Recreation    Capricorn  01,August,2004   \n",
       "1628   589736    male   35         Technology        Aries  05,August,2004   \n",
       "\n",
       "                                                   text  \n",
       "4623          Party Pooper  I have been staying up l...  \n",
       "2201             yay!  the blog is finally posting. ...  \n",
       "3849             New blog created where me and Jrago...  \n",
       "3929                   damn it...April's been medita...  \n",
       "485          Do you remember watching music videos? ...  \n",
       "889              i am sorry for the redundancy (iron...  \n",
       "1628                                 woop woop           "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blogs  = pd.read_csv('E:\\\\RNN\\\\blogs.zip', compression='zip', header=0, sep=',', quotechar='\"') # reading the content directly from zip file\n",
    "\n",
    "\n",
    "blogs = pd.read_csv('E:\\\\RNN\\\\blogs\\\\Output_1.csv')\n",
    "blogs.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        A. Clearly write outcome of data analysis(Minimum 2 points) [2 Marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'gender', 'age', 'topic', 'sign', 'date', 'text'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>573</td>\n",
       "      <td>4978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td>urlLink    urlLink audblog audio post ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2332</td>\n",
       "      <td>2483</td>\n",
       "      <td>2311</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.532153e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.544600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.302576e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.766008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.677050e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.897360e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.497900e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.168577e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.321554e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id gender          age       topic   sign            date  \\\n",
       "count   5.000000e+03   5000  5000.000000        5000   5000            5000   \n",
       "unique           NaN      2          NaN          20     12             573   \n",
       "top              NaN   male          NaN  Technology  Aries  05,August,2004   \n",
       "freq             NaN   3294          NaN        2332   2483            2311   \n",
       "mean    1.532153e+06    NaN    29.544600         NaN    NaN             NaN   \n",
       "std     1.302576e+06    NaN     7.766008         NaN    NaN             NaN   \n",
       "min     4.677050e+05    NaN    14.000000         NaN    NaN             NaN   \n",
       "25%     5.897360e+05    NaN    24.000000         NaN    NaN             NaN   \n",
       "50%     6.497900e+05    NaN    35.000000         NaN    NaN             NaN   \n",
       "75%     3.168577e+06    NaN    35.000000         NaN    NaN             NaN   \n",
       "max     4.321554e+06    NaN    46.000000         NaN    NaN             NaN   \n",
       "\n",
       "                                                     text  \n",
       "count                                                5000  \n",
       "unique                                               4978  \n",
       "top             urlLink    urlLink audblog audio post ...  \n",
       "freq                                                    7  \n",
       "mean                                                  NaN  \n",
       "std                                                   NaN  \n",
       "min                                                   NaN  \n",
       "25%                                                   NaN  \n",
       "50%                                                   NaN  \n",
       "75%                                                   NaN  \n",
       "max                                                   NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         int64\n",
       "gender    object\n",
       "age        int64\n",
       "topic     object\n",
       "sign      object\n",
       "date      object\n",
       "text      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        B. Clean the Structured Data [3 Marks]\n",
    "                i. Missing value analysis and imputation. [1 Marks]\n",
    "                ii. Eliminate Non-English textual data. [2 Marks]\n",
    "                Hint: Refer ‘langdetect’ library to detect language of the input text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing value \n",
    "blogs.isnull().sum()  # from the count in describe also we can say that there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Blogs before eliminating Non-English Text (5000, 7)\n",
      "Shape of Blogs after eleminating Non-English Text (4737, 7)\n",
      "Number of English Blogs 4725\n",
      "Number of other Blogs 263\n"
     ]
    }
   ],
   "source": [
    "# Eliminate Non-English textual data using langdetect. \n",
    "print(\"Shape of Blogs before eliminating Non-English Text\", blogs.shape)\n",
    "\n",
    "loc = 0\n",
    "eng = 0\n",
    "other =0\n",
    "for blog in blogs['text']:\n",
    "    \n",
    "    try:\n",
    "        if (detect(blog) == 'en'):\n",
    "            eng = eng + 1\n",
    "        else:\n",
    "            # print(loc)\n",
    "            blogs.drop(loc, inplace = True)\n",
    "            other = other +1\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "    loc = loc +1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Blogs after eleminating Non-English Text (4737, 7)\n",
      "Number of English Blogs 4725\n",
      "Number of other Blogs 263\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Blogs after eleminating Non-English Text\", blogs.shape)\n",
    "print(\"Number of English Blogs\", eng)\n",
    "print(\"Number of other Blogs\", other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>766556</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>03,March,2003</td>\n",
       "      <td>So much for romantic notions   urlLink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>3389671</td>\n",
       "      <td>male</td>\n",
       "      <td>26</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Leo</td>\n",
       "      <td>22,June,2004</td>\n",
       "      <td>I've been working on my new game for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td>(hopping up and down, clapping) cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td>that is the funniest thing i have h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>766556</td>\n",
       "      <td>female</td>\n",
       "      <td>34</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Sagittarius</td>\n",
       "      <td>13,August,2003</td>\n",
       "      <td>Automatic Flatterer  Of course, you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,August,2004</td>\n",
       "      <td>Korea's pretty funny sometimes.  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  gender  age              topic         sign            date  \\\n",
       "4340   766556  female   34             indUnk  Sagittarius   03,March,2003   \n",
       "262   3389671    male   26             indUnk          Leo    22,June,2004   \n",
       "2369   589736    male   35         Technology        Aries  05,August,2004   \n",
       "2100   589736    male   35         Technology        Aries  05,August,2004   \n",
       "4471   766556  female   34             indUnk  Sagittarius  13,August,2003   \n",
       "69    3581210    male   33  InvestmentBanking     Aquarius  10,August,2004   \n",
       "\n",
       "                                                   text  \n",
       "4340          So much for romantic notions   urlLink...  \n",
       "262          I've been working on my new game for a ...  \n",
       "2369             (hopping up and down, clapping) cel...  \n",
       "2100             that is the funniest thing i have h...  \n",
       "4471          Automatic Flatterer  Of course, you ca...  \n",
       "69                 Korea's pretty funny sometimes.  ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4737, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess unstructured data to make it consumable for model training. [5 Marks]\n",
    "        A. Eliminate All special Characters and Numbers [2 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_html_tags(text):\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "#     stripped_text = soup.get_text()\n",
    "#     return stripped_text\n",
    "\n",
    "# strip_html_tags('<html><h2>Some important text</h2></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a sentence like (I'll be a Father). If we remove the special characters before contractions, then it becomes 'Ill be a Father'. Which actually changes the entire meaning.\n",
    "# So lets apply contractions first and then remove special characters.\n",
    "\n",
    "def expand_contractions(text,contraction_mapping=contractions_dict):\n",
    "    contractions_pattern=re.compile('({})'.format('|'.join\n",
    "                                      (contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match=contraction.group(0)\n",
    "        first_char=match[0]\n",
    "        expanded_contraction=contraction_mapping.get(match)\\\n",
    "            if contraction_mapping.get(match)\\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction=first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction   \n",
    "    \n",
    "    \n",
    "    try:                   \n",
    "        expanded_text=contractions_pattern.sub(expand_match,str(text))\n",
    "        expanded_text=re.sub(\"'\",\" \",expanded_text)\n",
    "    except:\n",
    "        return text\n",
    "    return expanded_text\n",
    "\n",
    "def remove_spl_char_n_numbers(text, remove_digits=True):\n",
    "    # text = expand_contractions(text)\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>text_new3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1103575</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>So... I had another one of those dreams...</td>\n",
       "      <td>So... I had another one of those dreams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1103575</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>mmm... strawberry tea for breakfast. To...</td>\n",
       "      <td>mmm... strawberry tea for breakfast. To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1103575</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Yay for a new layout!!  Yeah, I know, I...</td>\n",
       "      <td>Yay for a new layout!!  Yeah, I know, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1103575</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Ok, so I lied... Fed up isn't playing F...</td>\n",
       "      <td>Ok, so I lied... Fed up is not playing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1103575</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>well, today I went to church and talked...</td>\n",
       "      <td>well, today I went to church and talked...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   topic                                               text  \\\n",
       "4995  1103575  indUnk         So... I had another one of those dreams...   \n",
       "4996  1103575  indUnk         mmm... strawberry tea for breakfast. To...   \n",
       "4997  1103575  indUnk         Yay for a new layout!!  Yeah, I know, I...   \n",
       "4998  1103575  indUnk         Ok, so I lied... Fed up isn't playing F...   \n",
       "4999  1103575  indUnk         well, today I went to church and talked...   \n",
       "\n",
       "                                              text_new3  \n",
       "4995         So... I had another one of those dreams...  \n",
       "4996         mmm... strawberry tea for breakfast. To...  \n",
       "4997         Yay for a new layout!!  Yeah, I know, I...  \n",
       "4998         Ok, so I lied... Fed up is not playing ...  \n",
       "4999         well, today I went to church and talked...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>text_new3</th>\n",
       "      <th>text_new2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4871</th>\n",
       "      <td>1233335</td>\n",
       "      <td>BusinessServices</td>\n",
       "      <td>drove 30 minutes north stood in a frien...</td>\n",
       "      <td>drove  minutes north stood in a friend ...</td>\n",
       "      <td>drove  minutes north stood in a friends...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>3022585</td>\n",
       "      <td>Education</td>\n",
       "      <td>At one point during every season, save ...</td>\n",
       "      <td>At one point during every season save f...</td>\n",
       "      <td>At one point during every season save f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>3413825</td>\n",
       "      <td>Student</td>\n",
       "      <td>Wow. it is gorgeous outside! Abso...</td>\n",
       "      <td>Wow it is gorgeous outside Absolu...</td>\n",
       "      <td>Wow it is gorgeous outside Absolu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Kvetchers Unite  Ever notice how some ...</td>\n",
       "      <td>Kvetchers Unite  Ever notice how some ...</td>\n",
       "      <td>Kvetchers Unite  Ever notice how some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Better   Something about Autumn air ma...</td>\n",
       "      <td>Better   Something about Autumn air ma...</td>\n",
       "      <td>Better   Something about Autumn air ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>1103575</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>this shade of red looks more and more l...</td>\n",
       "      <td>this shade of red looks more and more l...</td>\n",
       "      <td>this shade of red looks more and more l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Love, support, and a lot of antibio...</td>\n",
       "      <td>Love support and a lot of antibioti...</td>\n",
       "      <td>Love support and a lot of antibioti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>So now I'm Dorothy  And apparently, my...</td>\n",
       "      <td>So now I am Dorothy  And apparently my...</td>\n",
       "      <td>So now Im Dorothy  And apparently my r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>1103575</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>oh, and I forgot...  last night at cant...</td>\n",
       "      <td>oh and I forgot  last night at cantor p...</td>\n",
       "      <td>oh and I forgot  last night at cantor p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>You know that feeling you get when you ...</td>\n",
       "      <td>You know that feeling you get when you ...</td>\n",
       "      <td>You know that feeling you get when you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4542</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Screw up bird   Apparently, yesterday ...</td>\n",
       "      <td>Screw up bird   Apparently yesterday w...</td>\n",
       "      <td>Screw up bird   Apparently yesterday w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>1550279</td>\n",
       "      <td>Student</td>\n",
       "      <td>Yesterday marked the one month po...</td>\n",
       "      <td>Yesterday marked the one month po...</td>\n",
       "      <td>Yesterday marked the one month po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>check out ben's picture on the sbac...</td>\n",
       "      <td>check out ben s picture on the sbac...</td>\n",
       "      <td>check out bens picture on the sbac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>If I adopted a new creed every time...</td>\n",
       "      <td>If I adopted a new creed every time...</td>\n",
       "      <td>If I adopted a new creed every time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>3176655</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Well I'm back at work.  Fuck ...</td>\n",
       "      <td>Well Im back at work  Fuck al...</td>\n",
       "      <td>Well Im back at work  Fuck al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>oh wow, that' s really sad about hi...</td>\n",
       "      <td>oh wow that s really sad about his ...</td>\n",
       "      <td>oh wow that s really sad about his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>3176655</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>Okay so I got to thinking you...</td>\n",
       "      <td>Okay so I got to thinking you...</td>\n",
       "      <td>Okay so I got to thinking you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>'This is a fancy way of racial prof...</td>\n",
       "      <td>This is a fancy way of racial prof...</td>\n",
       "      <td>This is a fancy way of racial profi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             topic  \\\n",
       "4871  1233335  BusinessServices   \n",
       "514   3022585         Education   \n",
       "1374  3413825           Student   \n",
       "4351   766556            indUnk   \n",
       "4516   766556            indUnk   \n",
       "4972  1103575            indUnk   \n",
       "1655   589736        Technology   \n",
       "4271   766556            indUnk   \n",
       "4987  1103575            indUnk   \n",
       "4497   766556            indUnk   \n",
       "4542   766556            indUnk   \n",
       "842   1550279           Student   \n",
       "1487   589736        Technology   \n",
       "3220   589736        Technology   \n",
       "598   3176655       Engineering   \n",
       "1579   589736        Technology   \n",
       "603   3176655       Engineering   \n",
       "1700   589736        Technology   \n",
       "\n",
       "                                                   text  \\\n",
       "4871         drove 30 minutes north stood in a frien...   \n",
       "514          At one point during every season, save ...   \n",
       "1374               Wow. it is gorgeous outside! Abso...   \n",
       "4351          Kvetchers Unite  Ever notice how some ...   \n",
       "4516          Better   Something about Autumn air ma...   \n",
       "4972         this shade of red looks more and more l...   \n",
       "1655             Love, support, and a lot of antibio...   \n",
       "4271          So now I'm Dorothy  And apparently, my...   \n",
       "4987         oh, and I forgot...  last night at cant...   \n",
       "4497         You know that feeling you get when you ...   \n",
       "4542          Screw up bird   Apparently, yesterday ...   \n",
       "842                Yesterday marked the one month po...   \n",
       "1487             check out ben's picture on the sbac...   \n",
       "3220             If I adopted a new creed every time...   \n",
       "598                    Well I'm back at work.  Fuck ...   \n",
       "1579             oh wow, that' s really sad about hi...   \n",
       "603                    Okay so I got to thinking you...   \n",
       "1700             'This is a fancy way of racial prof...   \n",
       "\n",
       "                                              text_new3  \\\n",
       "4871         drove  minutes north stood in a friend ...   \n",
       "514          At one point during every season save f...   \n",
       "1374               Wow it is gorgeous outside Absolu...   \n",
       "4351          Kvetchers Unite  Ever notice how some ...   \n",
       "4516          Better   Something about Autumn air ma...   \n",
       "4972         this shade of red looks more and more l...   \n",
       "1655             Love support and a lot of antibioti...   \n",
       "4271          So now I am Dorothy  And apparently my...   \n",
       "4987         oh and I forgot  last night at cantor p...   \n",
       "4497         You know that feeling you get when you ...   \n",
       "4542          Screw up bird   Apparently yesterday w...   \n",
       "842                Yesterday marked the one month po...   \n",
       "1487             check out ben s picture on the sbac...   \n",
       "3220             If I adopted a new creed every time...   \n",
       "598                    Well Im back at work  Fuck al...   \n",
       "1579             oh wow that s really sad about his ...   \n",
       "603                    Okay so I got to thinking you...   \n",
       "1700              This is a fancy way of racial prof...   \n",
       "\n",
       "                                              text_new2  \n",
       "4871         drove  minutes north stood in a friends...  \n",
       "514          At one point during every season save f...  \n",
       "1374               Wow it is gorgeous outside Absolu...  \n",
       "4351          Kvetchers Unite  Ever notice how some ...  \n",
       "4516          Better   Something about Autumn air ma...  \n",
       "4972         this shade of red looks more and more l...  \n",
       "1655             Love support and a lot of antibioti...  \n",
       "4271          So now Im Dorothy  And apparently my r...  \n",
       "4987         oh and I forgot  last night at cantor p...  \n",
       "4497         You know that feeling you get when you ...  \n",
       "4542          Screw up bird   Apparently yesterday w...  \n",
       "842                Yesterday marked the one month po...  \n",
       "1487             check out bens picture on the sbac ...  \n",
       "3220             If I adopted a new creed every time...  \n",
       "598                    Well Im back at work  Fuck al...  \n",
       "1579             oh wow that s really sad about his ...  \n",
       "603                    Okay so I got to thinking you...  \n",
       "1700             This is a fancy way of racial profi...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i = 0\n",
    "# for blog in blogs['text']:\n",
    "#     blog = remove_spl_char_n_numbers(blog)\n",
    "    \n",
    "\n",
    "blogs['text_new3'] = blogs['text'].apply(expand_contractions)\n",
    "blogs['text_new3'] = blogs['text_new3'].apply(remove_spl_char_n_numbers)\n",
    "blogs.sample(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blogs.drop(columns=[\"gender\", 'age','sign','date','text_new'],inplace=True)\n",
    "# blogs.drop(columns=['text_new2'],inplace=True)\n",
    "# blogs.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'll probably be moving to canada  \""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_contractions(\"i'll probably be moving to canada  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unicodedata\n",
    "# def remove_accented_chars(text):\n",
    "#     text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#     return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_stemmer(text):\n",
    "#     ps = nltk.porter.PorterStemmer()\n",
    "#     text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "#     return text\n",
    "\n",
    "# simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")\n",
    "\n",
    "\n",
    "\n",
    "# def lemmatize_text(text):\n",
    "#     text = nlp(text)\n",
    "#     text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "#     return text\n",
    "\n",
    "# lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")\n",
    "\n",
    "\n",
    "# def remove_stopwords(text, is_lower_case=False):\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     tokens = [token.strip() for token in tokens]\n",
    "#     if is_lower_case:\n",
    "#         filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "#     else:\n",
    "#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "#     filtered_text = ' '.join(filtered_tokens)    \n",
    "#     return filtered_text\n",
    "\n",
    "# remove_stopwords(\"The, and, if are stopwords, computer is not\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        B. Lowercase all textual data [1 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>text_new3</th>\n",
       "      <th>text_new2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>3022585</td>\n",
       "      <td>Education</td>\n",
       "      <td>When riding my bike and a slight hill a...</td>\n",
       "      <td>when riding my bike and a slight hill a...</td>\n",
       "      <td>When riding my bike and a slight hill a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>3912317</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>'Intelligence is an extremely subtle co...</td>\n",
       "      <td>intelligence is an extrehemely subtle ...</td>\n",
       "      <td>Intelligence is an extremely subtle con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>I doubt that a negative self-image ...</td>\n",
       "      <td>i doubt that a negative selfimage i...</td>\n",
       "      <td>I doubt that a negative selfimage i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3429420</td>\n",
       "      <td>Student</td>\n",
       "      <td>'The hierachical order'  Blades tore sk...</td>\n",
       "      <td>the hierachical order   blades tore sk...</td>\n",
       "      <td>The hierachical order  Blades tore skie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       topic                                               text  \\\n",
       "480   3022585   Education         When riding my bike and a slight hill a...   \n",
       "4043  3912317      indUnk         'Intelligence is an extremely subtle co...   \n",
       "2468   589736  Technology             I doubt that a negative self-image ...   \n",
       "228   3429420     Student         'The hierachical order'  Blades tore sk...   \n",
       "\n",
       "                                              text_new3  \\\n",
       "480          when riding my bike and a slight hill a...   \n",
       "4043          intelligence is an extrehemely subtle ...   \n",
       "2468             i doubt that a negative selfimage i...   \n",
       "228           the hierachical order   blades tore sk...   \n",
       "\n",
       "                                              text_new2  \n",
       "480          When riding my bike and a slight hill a...  \n",
       "4043         Intelligence is an extremely subtle con...  \n",
       "2468             I doubt that a negative selfimage i...  \n",
       "228          The hierachical order  Blades tore skie...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs['text_new3'] = blogs['text_new3'].apply(lambda x:x.lower())\n",
    "blogs.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        C. Remove all Stopwords [1 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>text_new3</th>\n",
       "      <th>text_new2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>New Favorite Bumpersticker   Seen on t...</td>\n",
       "      <td>new favorite bumpersticker seen bumper shiny v...</td>\n",
       "      <td>New Favorite Bumpersticker   Seen on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>3022585</td>\n",
       "      <td>Education</td>\n",
       "      <td>Why people?  Again, google has sent som...</td>\n",
       "      <td>people google sent someone site typed together...</td>\n",
       "      <td>Why people  Again google has sent someo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>i want to see more chickens in spac...</td>\n",
       "      <td>want see chickens space</td>\n",
       "      <td>i want to see more chickens in spac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>they really do  urlLink hate  us   ...</td>\n",
       "      <td>really urllink hate us</td>\n",
       "      <td>they really do  urlLink hate  us   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       topic                                               text  \\\n",
       "4519   766556      indUnk          New Favorite Bumpersticker   Seen on t...   \n",
       "542   3022585   Education         Why people?  Again, google has sent som...   \n",
       "3434   589736  Technology             i want to see more chickens in spac...   \n",
       "3180   589736  Technology             they really do  urlLink hate  us   ...   \n",
       "\n",
       "                                              text_new3  \\\n",
       "4519  new favorite bumpersticker seen bumper shiny v...   \n",
       "542   people google sent someone site typed together...   \n",
       "3434                            want see chickens space   \n",
       "3180                             really urllink hate us   \n",
       "\n",
       "                                              text_new2  \n",
       "4519          New Favorite Bumpersticker   Seen on t...  \n",
       "542          Why people  Again google has sent someo...  \n",
       "3434             i want to see more chickens in spac...  \n",
       "3180             they really do  urlLink hate  us   ...  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "blogs['text_new3'] = blogs['text_new3'].apply(remove_stopwords)\n",
    "blogs.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "        D. Remove all extra white spaces [1 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>text_new3</th>\n",
       "      <th>text_new2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>1415200</td>\n",
       "      <td>Student</td>\n",
       "      <td>This week has gone by really slow. But ...</td>\n",
       "      <td>week gone really slow today tomarrow boat load...</td>\n",
       "      <td>This week has gone by really slow But t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>3887270</td>\n",
       "      <td>Student</td>\n",
       "      <td>omg..i'm like FREAKED OUT..there ...</td>\n",
       "      <td>omgim like freaked outthere random guywho dont...</td>\n",
       "      <td>omgim like FREAKED OUTthere was t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3581210</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Ah,  urlLink the monsoon season h...</td>\n",
       "      <td>ah urllink monsoon season ended thus begins ur...</td>\n",
       "      <td>Ah  urlLink the monsoon season ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>3539003</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Stroke my face that's restin...</td>\n",
       "      <td>stroke face resting lap tell ok tell together ...</td>\n",
       "      <td>Stroke my face thats resting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>stubborn monkey indeed.  that artic...</td>\n",
       "      <td>stubborn monkey indeed article trappings onion...</td>\n",
       "      <td>stubborn monkey indeed  that articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>3168577</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>The Art of Photo(shop)graphy   Just pl...</td>\n",
       "      <td>art photoshopgraphy playind aroung photo urlli...</td>\n",
       "      <td>The Art of Photoshopgraphy   Just play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>nay bark grass go may straight chan...</td>\n",
       "      <td>nay bark grass go may straight chant faye k q ...</td>\n",
       "      <td>nay bark grass go may straight chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>589736</td>\n",
       "      <td>Technology</td>\n",
       "      <td>game was good!  we missed you and a...</td>\n",
       "      <td>game good missed angie tho</td>\n",
       "      <td>game was good  we missed you and an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Clean Up Time  New Year started, and I...</td>\n",
       "      <td>clean time new year started decided clean blog...</td>\n",
       "      <td>Clean Up Time  New Year started and I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>766556</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Sigh of relief  Well, thank god that's...</td>\n",
       "      <td>sigh relief well thank god christmas day histo...</td>\n",
       "      <td>Sigh of relief  Well thank god thats o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id              topic  \\\n",
       "1061  1415200            Student   \n",
       "3752  3887270            Student   \n",
       "51    3581210  InvestmentBanking   \n",
       "80    3539003             indUnk   \n",
       "2786   589736         Technology   \n",
       "809   3168577             indUnk   \n",
       "2325   589736         Technology   \n",
       "2993   589736         Technology   \n",
       "4283   766556             indUnk   \n",
       "4221   766556             indUnk   \n",
       "\n",
       "                                                   text  \\\n",
       "1061         This week has gone by really slow. But ...   \n",
       "3752               omg..i'm like FREAKED OUT..there ...   \n",
       "51                 Ah,  urlLink the monsoon season h...   \n",
       "80                      Stroke my face that's restin...   \n",
       "2786             stubborn monkey indeed.  that artic...   \n",
       "809           The Art of Photo(shop)graphy   Just pl...   \n",
       "2325             nay bark grass go may straight chan...   \n",
       "2993             game was good!  we missed you and a...   \n",
       "4283          Clean Up Time  New Year started, and I...   \n",
       "4221          Sigh of relief  Well, thank god that's...   \n",
       "\n",
       "                                              text_new3  \\\n",
       "1061  week gone really slow today tomarrow boat load...   \n",
       "3752  omgim like freaked outthere random guywho dont...   \n",
       "51    ah urllink monsoon season ended thus begins ur...   \n",
       "80    stroke face resting lap tell ok tell together ...   \n",
       "2786  stubborn monkey indeed article trappings onion...   \n",
       "809   art photoshopgraphy playind aroung photo urlli...   \n",
       "2325  nay bark grass go may straight chant faye k q ...   \n",
       "2993                         game good missed angie tho   \n",
       "4283  clean time new year started decided clean blog...   \n",
       "4221  sigh relief well thank god christmas day histo...   \n",
       "\n",
       "                                              text_new2  \n",
       "1061         This week has gone by really slow But t...  \n",
       "3752               omgim like FREAKED OUTthere was t...  \n",
       "51                 Ah  urlLink the monsoon season ha...  \n",
       "80                      Stroke my face thats resting...  \n",
       "2786             stubborn monkey indeed  that articl...  \n",
       "809           The Art of Photoshopgraphy   Just play...  \n",
       "2325             nay bark grass go may straight chan...  \n",
       "2993             game was good  we missed you and an...  \n",
       "4283          Clean Up Time  New Year started and I ...  \n",
       "4221          Sigh of relief  Well thank god thats o...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "blogs['text_new3'] = blogs['text_new3'].apply(remove_whitespaces)\n",
    "blogs.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blogs['text_new3'].str.replace(\"  \",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a base Classification model [8 Marks]\n",
    "        A. Create dependent and independent variables [2 Marks]\n",
    "        Hint: Treat ‘topic’ as a Target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since X = blogs['text'] and y =['topic']]\n",
    "\n",
    "x = blogs['text_new3']\n",
    "\n",
    "y = blogs['topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        B. Split data into train and test. [1 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test =train_test_split(x,y, random_state=369)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        C. Vectorize data using any one vectorizer. [2 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(x)\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        D. Build a base model for Supervised Learning - Classification. [2 Marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        E. Clearly print Performance Metrics. [1 Marks]\n",
    "        Hint: Accuracy, Precision, Recall, ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Improve Performance of model. [14 Marks]\n",
    "        A. Experiment with other vectorisers. [4 Marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        B. Build classifier Models using other algorithms than base model. [4 Marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        C. Tune Parameters/Hyperparameters of the model/s. [4 Marks]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        D. Clearly print Performance Metrics. [2 Marks]\n",
    "            Hint: Accuracy, Precision, Recall, ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Share insights on relative performance comparison [8 Marks]\n",
    "        A. Which vectorizer performed better? Probable reason?.[2 Marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        B. Which model outperformed? Probable reason? [2 Marks]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        C. Which parameter/hyperparameter significantly helped to improve performance?Probable reason?. [2 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        D. According to you, which performance metric should be given most importance, why?. [2 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # PART B - 20 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • DOMAIN: \n",
    "    Customer support\n",
    "### • CONTEXT: \n",
    "    Great Learning has a an academic support department which receives numerous support requests every day throughout the year. Teams are spread across geographies and try to provide support round the year. Sometimes there are circumstances where due to heavy workload certain request resolutions are delayed, impacting company’s business. Some of the requests are very generic where a proper resolution procedure delivered to the user can solve the problem. Company is looking forward to design an automation which can interact with the user, understand the problem and display the resolution procedure [ if found as a generic request ] or redirect the request to an actual human support executive if the request is complex or not in it’s database.\n",
    "### • DATA DESCRIPTION: \n",
    "    A sample corpus is attached for your reference. Please enhance/add more data to the corpus using your linguistics skills.\n",
    "### • PROJECT OBJECTIVE: \n",
    "    Design a python based interactive semi - rule based chatbot which can do the following:\n",
    "        1. Start chat session with greetings and ask what the user is looking for. [5 Marks]\n",
    "        2. Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus. [10 Marks]\n",
    "        3. End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user asks to end it. [5 Marks]\n",
    "        Hint: There are a lot of techniques using which one can clean and prepare the data which can be used to train a ML/DL classifier. Hence, it might require you to experiment, research, self learn and implement the above classifier. There might be many iterations between hand building the corpus and designing the best fit text classifier. As the quality and quantity of corpus increases the model’s performance i.e. ability to answer\n",
    "        right questions also increases.\n",
    "        Reference: https://www.mygreatlearning.com/blog/basics-of-building-an-artificial-intelligence-chatbot/\n",
    "### • Evaluation: \n",
    "    Evaluator will use linguistics to twist and turn sentences to ask questions on the topics described in DATA DESCRIPTION and check if the bot is giving relevant replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texttospeech' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18104/113463108.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtexttospeech\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'texttospeech' is not defined"
     ]
    }
   ],
   "source": [
    "texttospeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "text = pyttsx3.init()\n",
    "s = 'The code is Compiled'\n",
    "text.say(s)\n",
    "text.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
